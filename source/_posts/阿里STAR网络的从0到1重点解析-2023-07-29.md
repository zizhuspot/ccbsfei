---
title: 阿里STAR网络的从0到1重点解析
date: 2023-08-10 23:30:00
categories:
  - 排序模型
  - 多任务模型
tags:
  - MLT
  - 多任务模型
  - 预估模型 
description: 提出了一种使用单个模型服务于多种业务场景的任务
cover: https://cdn.jsdelivr.net/gh/1oscar/image_house@main/STAR.jpg
---

## 整体介绍

- 提出了一种使用单个模型服务于多种业务场景的任务。我们将其称之为 multi-domain CTR prediction，「即我们的模型需要同时预测在D1,D2,Dm业务场景下的点击率。模型以(x,y,p)作为输入，其中x为输入特征，y为点击标签，p为不同业务场景的标识」
- 这里指的多种ctr业务场景是针对首页推荐和猜你喜欢，特点都是同一个app内。
- 提出了不同业务场景下，数据互相共享互补提升的思路，提出了一种新的任务：multi-domain CTR prediction。并针对这类任务设计了PN，Star Topology FCN，辅助网络等结构


## 背景
传统方法中的一个模型对应一种业务, 这里只使用一种模型，便可以服务于多种CTR业务场景。好处：既可以减少多个模型带来的维护成本与计算资源，也可以共享不同业务场景下的数据(有些场景流量少，缺乏训练数据)。

## 原理

![原理1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729231840.png)

单场景CTR预估的方法将输入经过embedding层后，通过pooling/concat操作得到一维的向量表示后，通过BN层，经过一系列FC层，输出最后的结果.

**Star网络：**

1.	将BN(Batch Normalization)层替换为PN(Partitioned Normalization)层。 「Partitioned Normalization (PN)」: 可以针对不同业务场景下不同的数据分布做定制化归一化

2.	将FCN替换为Star Topology FCN。 「Star topology fully-connected neural network」: 文章提出了Star Topology Adaptive Recommender(STAR) 来解决多领域的CTR预估问题。该网络可以充分利用多个业务中的数据来提升各自业务的指标

3.	将domain indicator直接输入。 文章提出了一种「辅助网络」(auxiliary network)，直接以业务场景的标识(domain indicator)作为输入，来使得网络更好的感知不同场景下的数据分布


![Star网络：](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729231945.png)

![Star网络：1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729232001.png)


在经过PN层后，输出z’会作为Star Topology FCN的输入. Star Topology FCN包含一个所有领域共享的FCN和每个领域各自独立的FCN. 所有的FCN数量为M+1, M为domain的数量。FCN:全连接网络dnn


![Star网络：2](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729232028.png)

## 模型架构

![Star网络：3](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729232045.png)


Star Topology FCN中每个业务场景网络的权重由共享FCN和其domain-specific FCN的权重共同决定。共享FCN来决定每个领域中数据的共性，而domain-specific FCN习得不同领域数据之间分布的差异性。

![Star Topology](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729232126.png)

## 损失函数

![损失函数](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729232152.png)


## 实验结果
共19个domain的数据

![实验结果](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729232213.png)

文中对比的baseline模型大都是多任务学习模型，

multi-domain和multi-task之间的区别主要是：

multi-domain的模型大都解决的是不同domain的相同问题，如CTR预估，其label space是相同的；而multi-task一般解决的是相同domain内的不同任务，如CTR预估和CVR预估，其label space是不同的。

## 代码实现

Star层代码：

```python
import tensorflow as tf
from tensorflow.python.keras import backend as K
from tensorflow.python.keras.initializers import Zeros, glorot_normal
from tensorflow.python.keras.layers import Layer
from tensorflow.python.keras.regularizers import l2

def activation_layer(activation):
    if isinstance(activation, str):
        act_layer = tf.keras.layers.Activation(activation)
    elif issubclass(activation, Layer):
        act_layer = activation()
    else:
        raise ValueError(
            "Invalid activation,found %s.You should use a str or a Activation Layer Class." % (activation))
    return act_layer

class STAR(Layer):

    def __init__(self, hidden_units, num_domains, activation='relu', l2_reg=0, dropout_rate=0, use_bn=False, output_activation=None,
                 seed=1024, **kwargs):
        self.hidden_units = hidden_units
        self.num_domains = num_domains
        self.activation = activation
        self.l2_reg = l2_reg
        self.dropout_rate = dropout_rate
        self.use_bn = use_bn
        self.output_activation = output_activation
        self.seed = seed

        super(STAR, self).__init__(**kwargs)

    def build(self, input_shape):
        input_size = input_shape[-1]
        hidden_units = [int(input_size)] + list(self.hidden_units)
        ## 共享FCN权重
        self.shared_kernels = [self.add_weight(name='shared_kernel_' + str(i),
                                        shape=(
                                            hidden_units[i], hidden_units[i + 1]),
                                        initializer=glorot_normal(
                                            seed=self.seed),
                                        regularizer=l2(self.l2_reg),
                                        trainable=True) for i in range(len(self.hidden_units))]

        self.shared_bias = [self.add_weight(name='shared_bias_' + str(i),
                                     shape=(self.hidden_units[i],),
                                     initializer=Zeros(),
                                     trainable=True) for i in range(len(self.hidden_units))]
        ## domain-specific 权重
        self.domain_kernels = [[self.add_weight(name='domain_kernel_' + str(index) + str(i),
                                        shape=(
                                            hidden_units[i], hidden_units[i + 1]),
                                        initializer=glorot_normal(
                                            seed=self.seed),
                                        regularizer=l2(self.l2_reg),
                                        trainable=True) for i in range(len(self.hidden_units))] for index in range(self.num_domains)]

        self.domain_bias = [[self.add_weight(name='domain_bias_' + str(index) + str(i),
                                     shape=(self.hidden_units[i],),
                                     initializer=Zeros(),
                                     trainable=True) for i in range(len(self.hidden_units))] for index in range(self.num_domains)]

        self.activation_layers = [activation_layer(self.activation) for _ in range(len(self.hidden_units))]

        if self.output_activation:
            self.activation_layers[-1] = activation_layer(self.output_activation)

        super(STAR, self).build(input_shape)  # Be sure to call this somewhere!

    def call(self, inputs, domain_indicator, training=None, **kwargs):
        deep_input = inputs
        output_list = [inputs] * self.num_domains 
        for i in range(len(self.hidden_units)):
            for j in range(self.num_domains):
                # 网络的权重由共享FCN和其domain-specific FCN的权重共同决定
                output_list[j] = tf.nn.bias_add(tf.tensordot(
                    output_list[j], self.shared_kernels[i] * self.domain_kernels[j][i], axes=(-1, 0)), self.shared_bias[i] + self.domain_bias[j][i])

                try:
                    output_list[j] = self.activation_layers[i](output_list[j], training=training)
                except TypeError as e:  # TypeError: call() got an unexpected keyword argument 'training'
                    print("make sure the activation function use training flag properly", e)
                    output_list[j] = self.activation_layers[i](output_list[j])
        output = tf.reduce_sum(tf.stack(output_list, axis=1) * tf.expand_dims(domain_indicator,axis=-1), axis=1)

        return output

    def compute_output_shape(self, input_shape):
        if len(self.hidden_units) > 0:
            shape = input_shape[:-1] + (self.hidden_units[-1],)
        else:
            shape = input_shape

        return tuple(shape)

    def get_config(self, ):
        config = {'activation': self.activation, 'hidden_units': self.hidden_units,
                  'l2_reg': self.l2_reg, 'use_bn': self.use_bn, 'dropout_rate': self.dropout_rate,
                  'output_activation': self.output_activation, 'seed': self.seed}
        base_config = super(STAR, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


```