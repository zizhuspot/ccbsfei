---
title: DDPM 概率模型从0到1深度解析
date: 2023-07-29 17:21:00
categories:
  - AI绘画
tags:
  - 文生图
  - ai画图
  - DDPM
  - 扩散模型
  - 去噪自编码器 
description: 发现了扩散模型和用于训练马尔科夫过程的变分推理、去噪分数匹配、退火Langevin动态、自回归模型和渐进有损压缩之间的联系
cover: https://cdn.jsdelivr.net/gh/1oscar/image_house@main/DDPM.png
---


## 摘要和介绍

- 作者使用扩散概率模型获取高质量的图像合成结果，一类受非平衡热力学考虑启发的潜变量模型。通过训练根据扩散概率模型和Langevin 动力学去噪分数匹配之间的新连接设计的加权变分界限获得的。在无条件CIFAR10 数据集上，我们获得9.46 的Inception score分数和3.17 的最新FID 分数。在LSUN 256*256数据集上，获得的样本质量跟渐进式GAN接近。
- 扩散概率模型是使用变分推理训练的参数化马尔可夫链，产生在有限时间后匹配数据的样本。学习该链的转换以逆转扩散过程，扩散过程是马尔可夫链以与采样过程相反的方向逐渐向数据添加噪声，直到信号被破坏。当扩散由少量高斯噪声组成时，将采样链转换设置为条件高斯，允许简单的神经网络参数化。
- 扩散模型的采样过程是一种渐进式解码，类似于沿位排序的自回归解码。

- 扩散模型 包含的两个过程
- 前向扩散过程：
- 反向生成过程

![扩散模型](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729175755.png)


## 背景

- 公式1：逆向过程，被定义成马尔科夫链，从p(𝑥_𝑇 )=𝑁(𝑥_𝑇:0,I)
- 开始学习高斯转换。

![背景1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729175844.png)

- 公式2 ：扩散模型和其他隐变量模型区别是近似后验(前向/扩散过程)被定义成马尔科夫链，根据变分schedule逐步向数据中添加噪声。

![公式2](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729175909.png)

- 公式3 ：优化负对数似然函数的变分界限进行训练

![公式3](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729175936.png)

- 公式4 ：前向过程的显著特性是允许以闭式在任意时间步t采样xt
  𝛼_t≔1-𝛽_𝑡 , ¯(𝛼_𝑡 ):=∏2_(𝑠=1)^𝑡▒𝛼_𝑠 

![公式4](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729175959.png)

- 公式5,6,7 ： 使用SGD优化L的随机项来进行有效训练。通过降低方差来进一步提升，重写L如公式五，公式五通过KL散度去衡量p_𝜃  〖(𝑥〗_(𝑡−1) |𝑥_𝑡)和前向过程后验，当条件是公式6，7。所有的KL散度都是高斯之间的对比，因此可以使用RaoBlackwellized 的闭式表达式计算代替高方差的Monte Carlo估计。

![公式5,6,7 ](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729180030.png)

![公式1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729180041.png)


## 扩散模型和去噪自编码器

- 公式8： 扩散模式看起来是一个受限的潜变量模型，但他们允许很大的自由度。必须选择正向过程方差𝛽_t，模型架构和反向过程的高斯分布参数化。我们建立了扩散模型和去噪分数匹配之间的连接来为扩散模型做一个简单的加权的变分界限目标。
- 前向过程和LT：LT是一个常数，在训练过程中可以忽略

- 逆向过程和𝐿_(1:𝑇−1)：设置∑2_𝜃▒〖(𝑥_𝑡,𝑡)〗, 𝑥_0~𝑁(0,1),𝜇_𝜃 (𝑥_𝑡,𝑡),         ,p_𝜃(𝑥_(𝑡−1) |𝑥_𝑡)=N〖(𝑥〗_(𝑡−1);𝜇_𝜃 (𝑥_𝑡,𝑡),𝛿_𝑡^2 I)，L_(t−1) 公式重写如下：


![扩散模型和去噪自编码器](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729180212.png)


- 公式9,10,11：C是不依赖𝜃的常数，对𝜇_𝜃的参数化就是预测(𝜇_t ) ̃，根据等式4重新参数化来扩展公式8，使用前向过程后验公式7。
- 𝜇_𝜃 (𝑥_𝑡,𝑡)的参数化如公式11

![公式9,10,11](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729180240.png)

我们能够训练逆向过程均值函数逼近器𝜇_𝜃去预测(𝜇_t ) ̃，或者修改它的参数，我们能够预测∈。
∈_𝜃在Denoising Score Matching 里面是估计的梯度，而噪声∈就是带噪声数据分布的score,即概率密度梯度值。

![算法1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729222453.png)

![算法2](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729222518.png)


![抽样](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729222545.png)

![抽样2](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729222600.png)

图像数据由{0,1…255}线性缩放到[-1,1]的整数组成。确保了神经网络反向过程从标准的先验p(𝑥_𝑇)开始。为了获得离散的log似然，将反向过程的最后一项设置为从高斯N〖(𝑥〗_0;𝜇_𝜃 (𝑥_1,1),𝛿_1^2 I)导出的离散解码器。

D是数据维度，i是提取一个维度。类似于在VAE解码器和自回归中的离散连续分布 ，我们在这里的选择确保变分界限是离散数据的无损码长，不需要在数据中加入噪声或将缩放操作的雅可比矩阵合并到对数似然中。

- 根据逆向过程和解码器的定义，变分边界关于𝜃可微。我们发现在下面的变分界线上训练有利于提高样本质量。
- 简化目标（公式14）丢弃了方程式中的权重因数。加权变分界限强调重建的各个方面，相对于标准变分权限来说。
- 简化目标的扩散模型，会降低对应于小于t的各项损失的权重。这可以使网络在较大的T项下专注于更困难的去噪任务。



## 实验

![实验1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729222958.png)

表1显示了CIFAR10 上的Inception score，
FID 分数和负对数似然，FID 得分为3.17 时，
作者的无条件模型比文献中的大多数模型
获得更好的样本质量。


### Progressive coding 
- 训练和测试之间的差距最多为每维0.03 位，这与其他基于likelihood的模型报告的差距相当，并表明我们的扩散模型没有过度拟合；
- 因为我们的样本质量很高，因此扩散模型具有感应偏差，使他们成为出色的有损压缩器。
- 渐进式有损压缩可以通过引入反映方程式形式的渐进式有损代码来进一步探索模型的速率失真行为。

![图五](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729223119.png)

图五显示了:逆向过程中时间、速率和失真率的关系

在速率畸变图的低速率区域，畸变急剧减小; 这表明大部分比特确实被分配给了难以察觉的失真。

### Progressive generation

- 无条件CIFAR 10渐进式生成。从噪声图像到清晰的图像过程。
- 扩展样本和样本质量评估在图10和14中

![generation 1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729223215.png)


显示了随机预测x_0~p_0 〖(x〗_0|x_t),对于不同的t，x_t 被冻结。当t很小时，细节被保留下来，t很大时，大的特征被保留下来。
右下角是x_t，其他 来自于p_t 〖(x〗_0|x_t)的采样。

![显示1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729223325.png)


## 相关工作

- ε- 预测反向过程参数化在扩散模型和多个噪声水平上的去噪分数匹配与退火的Langevin 动力学之间建立了联系以进行采样。
- 该连接还具有相反的含义，即某种加权形式的去噪分数匹配与训练Langevin 样采样器的变分推理相同。
- 学习马尔可夫链转移算子的其他方法包括扩散训练，变分回退，生成随机网络和其他。
- 我们的工作对基于能量的模型的其他最新工作产生影响。我们的速率- 失真曲线是在变分界限的一次评估中随时间计算的。

## 结论


- 我们发现了扩散模型和用于训练马尔科夫过程的变分推理、去噪分数匹配、退火Langevin动态、自回归模型和渐进有损压缩之间的联系。
- 扩散模型对图像数据具有良好的归纳偏差，我们期待研究它们在其他数据模态和其他类型的生成模型和机器学习系统中的效用。
- 生成模型有很多恶意用途，比如用于政治目的的使用假图像。
- 扩散模型可能对数据压缩有用。



## 附录ABCDE

- 神经网络架构遵循PixelCNN + + 的主干，32*32模型使用4种特征映射分辨率，256*256模型使用6种。作者的CIFAR10 模型有3570 万个参数，LSUN 和CelebA-HQ 模型有114 万个参数。作者还通过增加滤波器计数训练了LSUN Bedroom模型的较大变体，参数约为256 万。
- 作者使用TPU v3-8 （类似于8 个v100 GPU ）进行实验。CIFAR 模型以每秒21 步的速度训练，batchsize=128 （10.6 小时训练到800k 步骤），采样一批256 个图像需要17 秒。
- 作者的CelebA-HQ / LSUN （〖256〗^2）模型在batch size=64 时以每秒2.2 步的速度训练，采样128 个图像需要300 秒。作者在CelebA-HQ 上训练了0.5M 步，LSUN 训练了2.4M 步，LSUN Cat 训练了1.8M 步，LSUN Church 训练了1.2M 步，较大的LSUN Bedroom模型训练了1.15M 步。




