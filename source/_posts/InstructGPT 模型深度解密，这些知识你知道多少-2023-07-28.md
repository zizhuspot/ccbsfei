---
title: InstructGPT 模型深度解密
date: 2023-07-30 11:30:00
categories:
  - 大模型
tags:
  - GPT1
  - GPT2
  - GPT3
  - InstructGPT
  - 强化学习
description: 使用PPO来微调SFT模型。输入一个prompt期望得到一个输出。给定一个prompt和response，生成奖励分数。除此之外，增加了KL散度降低奖励模型的过度优化。我们称这个模型为PPO。
作者把预训练的梯度加入到PPO的梯度中，为了缓和模型在公开数据集中的性能损失。我们称这个模型为PPO-ptx。
cover: https://cdn.jsdelivr.net/gh/1oscar/image_house@main/10021.jpg
---

## 摘要&介绍

### ChatGPT 发展时间线

![ChatGPT发展时间线](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729092614.png)

把语言模型变大并不代表说会按照人类意图工作，大的语言模型可能会生成不真实，有毒或没有帮助的一些答案。换句话说就是模型没有跟用户align在一起。这篇论文中作者使用用户反馈的形式对模型进行了微调。

刚开始使用了一组人工标注的提示词使用gpt-3进行有监督微调，第二步收集模型输出的有序的数据集，从人类反馈中使用强化学习（RLHF）进一步fine-tune有监督模型 。 这个结果模型我们称为instruct GPT。

在人类的评估中1.3B的instruct GPT模型输出优于175B的gpt-3模型的输出，虽然instruct  gpt也会犯简单的错误，但结果表明使用人类反馈进行微调是一个正确的方向来使模型输出和人类意图进行align。


### 人类在不同模型上的评估，
GPT（prompt）：是GPT3在prompt上做比较多的调整

SFT：第一个模型

PPO：当γ=0时，这个模型叫做PPO；

PPO-ptx：当γ不为0时，这个模型叫做PPO-ptx。

![人类在不同模型上的评估，](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729092703.png)


语言模型经常编造事实，生成带有偏见和有毒的文本或者没有遵循用户的指令的原因是语言模型的目标函数用于预测下一个token-这与我们的目标不同，我们的目标是生成遵循用户指令的有用的和安全的结果。

作者aligning语言模型方法（three models）：

1)从人类反馈中使用增强学习（RLHF）来微调GPT-3，具体的做法是人工写了很多prompt，用标注工具把答案写出来，这样就标注了一个数据集，然后对GPT3模型做微调，结果是SFT（Supervised Fine-Tuning)。

2) 使用SFT模型，输出多个结果，对结果进行排序(构建新的数据集)，训练一个奖励模型（RM）（人工标注太贵）。

3)使用RM作为奖励函数，fine-tune有监督学习（SFT），使用PPO算法来最大化奖励函数。这个模型称为instruct GPT。

![语言模型方法](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729092746.png)

作者训练了三个模型（1.3B，6B，175B），这些模型都使用GPT-3框架。

作者发现如下：

    与GPT-3的输出相比，标注者明显更喜欢InstructGPT的输出
    与GPT-3相比，InstructGPT模型的真实性要好一些。
    与GPT-3相比，InstructGPT在毒性方面略有改善，因为它可以说我不想回答你这个问题，但是偏见上没太多提升。
    微调的时候都是针对某些任务做微调，可能使得你在一些别的任务上性能会下降。
    没造过训练数据的雇员觉得InstructGPT效果更好；
    公开的NLP数据集不能反应我们的语言模型是如何被使用的；
    InstructGPT模型展示了更好的泛化性在那些RLHF微调之外的指令上。
    InstructGPT仍然会犯一些简单的错误。


## 相关工作

    从人的反馈中学习和对齐
    作者使用基于人类反馈的强化学习(RLHF)进行微调
    训练语言模型来遵循指令。
    在公共NP数据集微调，在不同的NLP数据集进行评估。结论是在一系列NLP任务上微调LMs，提高了他们在执行任务时的表现，无论是在zero-shot还是few-shot上。
    评估语言模型的危害；
    有一个新兴的不断发展的领域，旨在建立基准，具体评估这些危害，但取得进展很难。
    修改语言模型的行为以减轻危害。
    在过滤的数据集上训练后，LMs产生的有害文本较少，但代价是语言建模性能略有下降。Xu使用多种方法来提高聊天机器人的安全性，包括数据过滤、在生成过程中阻止某些单词或 n-gram、指定token， 和human-in-the-loop 数据收集等。



## 方法和实验细节

    作者的方法遵循了Ziegler et al.和Stiennon et al.的方法，从一个预训练的语言模型、一个希望模型产生align输出的提示语和一组经过培训的标注者开始。执行下面三个步骤
    Step1：收集数据，并训练一个监督模型。使用监督学习在这些数据上微调一个预训练好的GPT-3模型。
    Step2：收集比较数据，并训练一个奖励模型。收集一些模型输出之间的比较数据，其中标注者指出他们对于给定输入更喜欢哪个输出。然后训练一个奖励模型来预测人类偏好的输出。
    Step3：使用PPO算法优化奖励模型下的策略。把奖励模型的输出作为一个标量奖励，用PPO算法在这个奖励下微调监督策略。第二步和第三步可以不断迭代；收集更多当前最佳策略下的比较数据，用它们来训练一个新的奖励模型和一个新的策略。

--- 
    提示语数据集主要是由提交到open AI API 上的文本提示语组成。没有使用来自使用API的客户的数据，对数据集删除重复的提示词，限制提示词的长度为200，过滤训练集中个人身份信息等。训练InstructGPT模型时，要求标注员写下面三类提示词：
    Plain:我们只是要求标注员写一个任意的问题，同时确保问题有足够的多样性。 
    Few-shot:我们要求标注者提出一个指令，以及该指令的多个查询/响应对。 
    User-based：我们在OpenAI的候选名单应用程序中列出了许多用例API。我们要求标注者提出与这些用例相对应的提示词。
    SFT数据集：13K条数据；RM数据集：33K条数据；PPO数据集：31K条数据

---

    训练任务来自两个来源:(1)由我们的标注者编写的提示数据集和（2）在我们的API上提供给早期InstructGPT模型的提示语数据集。
    提示语非常多样化，包括生成、问答、对话、摘要、摘录等其他自然语言任务。我们的数据集超过96%是英语。
    尽管任务很复杂，但我们发现标注者之间的一致率相当高：训练标注者在 72.6 ± 1.5% 的时间内彼此一致，而对于hold-out者，这个数字是 77.3 ± 1.3%。为了比较，研究人员之间的一致性为 73 ± 4%。
    我们从GPT-3预训练语言模型开始。我们用三个不同的技术来训练模型:
    Supervised fine-tuning（SFT）。监督微调(SFT)。我们在我们的标注数据上微调GPT-3使用监督学。我们训练了16个epoch，使用余弦学习率衰减，残差为0.2。我们根据验证集上的RM分数进行最终的SFT模型选择。我们发现我们的SFT模型在1 epoch后的验证集的损失上过拟合;然而，我们发现尽管存在过拟合，但更多epoch的训练对RM分数和人类偏好评级都有帮助。


**Reward modeling（RM）奖励建模(RM)**

SFT模型去掉最后的umembedding layer，输入提示语和答案，输出一个标量结果（reward）。在这篇论文中我们仅使用了6B RMs, 这节省了很大的计算量。我们发现175B RM训练不稳定因此不适合用于值函数，在RL期间。
奖励模型的损失函数是：

![奖励建模(RM)](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729093037.png)


r_𝜃 (𝑥,𝑦)是奖励模型对于提示词x和完成y的标量输出，y_w是y_w和y_l中更受欢迎的补全，D是人类比较的数据集。
为了加快比较收集，取K = 9，36对排序进行优化

**Reinforcement learning (RL)  强化学习(RL)**

作者使用PPO来微调SFT模型。输入一个prompt期望得到一个输出。给定一个prompt和response，生成奖励分数。除此之外，增加了KL散度降低奖励模型的过度优化。我们称这个模型为PPO。
作者把预训练的梯度加入到PPO的梯度中，为了缓和模型在公开数据集中的性能损失。我们称这个模型为PPO-ptx。
我们在强化学习中最大化这个目标函数。

![强化学习(RL)](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729093148.png)

**Baselines：**

我们对PPO模型、SFT模型和GPT-3的性能进行比较。我们也对比了给gpt-3的提示词提供few-shot 前缀的模型。前缀附加在用户指定的指令之前。

还对比了InstructGPT与在FLAN和T0上的微调175B GPT-3模型。两个数据集都由各种NLP任务组成，结合自然语言指令。我们分别在大约100万个例子上对它们进行微调并选择在验证集上获得最高奖励模型分数选为checkpoint。

评估我们的模型是如何“对齐”的，定量评估分为两个独立的部分

    API评价：主要的评价标准是人类偏好胜率
    对公共NLP数据集的评估

## 结果

    图3:我们的模型的偏好结果，通过对比175B SFT模型的胜率。
    我们从对提交给GPT-3模型(左)的提示的评估中省略了GPT(prompt) 
    因为这些提示已经设计得很好，可以适用 于GPT-3， 和InstructGPT 模型得提示恰好相反（右）。

![结果](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729093256.png)

    图4中，展示了标注者也对InstructGPT输出进行了更具体的评价轴。具体来说，与GPT-3相比，InstructGPT 输出更接近一个客户助理，更经常遵循指令中明确定义的约束，更少的有害性。

![结果1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729093424.png)



## 讨论

    在这项工作中，我们的对齐研究方法是迭代的:我们正在改进的是现有的AI系统的对齐而不是抽象地专注于调整尚不存在的人工智能系统。
    作者在研究中吸取的经验教训：
        增加模型对齐的成本相对于预训练是更划算的。
        我们已经看到一些证据表明，InstructGPT将“遵循指令”泛化到我们不对其进行监督的设置，例如非英语语言任务和与代码相关的任务。
        我们能够减轻通过微调带来的大多数性能下降。
        我们已经从现实世界的研究中验证了对齐技术。

---

    局限性
    InstructGPT模型的行为部分是由人类的反馈决定的（标注员）。一些标签任务依赖于价值判断，这可能是受我们承包商的身份，信仰，文化背景和个人经历的影响。 
    我们的模型既不完全对齐，也不完全安全;他们仍然产生有毒或偏见输出，编造事实，在没有明确提示的情况下产生性和暴力内容。
    开放问题
    可以使用对抗设置来减少有害的输出；
    比较也不一定是提供对齐信号的最有效方式。
    另一个可能改进我们方法的修改是过滤预训练混合数据中的有毒内容或使用合成指令扩充此数据。


## 附录

### 标注员写提示词的三种类型
Plain：我们只是要求标记者提出一个任意任务，同时确保任务的多样性。
Few-shot：我们要求标注者提出一条指令，以及该指令的多个查询/响应对。
User-based：我们在 OpenAI API 的应用程序中陈述了许多用例。 我们要求标注者提出与这些用例相对应的提示。
### API 用户提示词
使用Instruct GPT模型早期版本用户提交的提示词。为了保证提示词的多样性，我们会删除重复的提示词，限制提示词的长度为200。除此之外，我们基于组织id来划分训练集，测试和验证集。
图表展示了一些用户的prompts：生成(generation)、开放式(open) QA、封闭式(closed) QA、头脑风暴、聊天(chat)、重写、总结、分类、提取或其他。
Table6 展示了用于训练/验证 SFT、RM 和 RL 模型的数据集的大小
Table7展示了数据集的多样性。
Table8展示了每个用户的平均prompts数量；
Table9展示了数据集分类的提示词长度，类别分类的提示词长度

### 模型细节

#### 所有模型都使用了GPT-3架构。所有模型训练使用Adam优化器。
1.SFT训练细节
SFT模型训练16个epoch，dropout是0.2，学习率是原始值的10%，没有学习率预热。1.3B和6B模型，使用LR值9.65e-6，batchsize=32；175B使用LR值5.03e-6，batchsize=8。最终模型根据RM分数进行选择的。
#### RM训练细节
选择6B RM模型的原因：大模型不稳定。RM模型训练过程中，对epoch敏感，多个epoch很快就过拟合，这里作者只训练了一个epoch，学习率9e-6，batchsize=64。每个prompt选择k=4和k=9标签补全，结果会有(K¦2)个对比项，因此单批最多可以包含64*(K¦2)≤2304 个对比项。

#### RLHF模型的初始化细节
从预训练的GPT-3模型初始化RLHF模型，在数据集上进行监督微调 2个epoch。
微调期间混入了10%的预训练数据，因为发现这对PPO训练很有帮助。
使用了1.3B和6B的batchsize=32；175B的batchsize=8。作者对比了每个模型的不同的峰值的学习率，选择了在演示和预训练验证集上损失都比较小的那个。

#### RLHF训练的细节
所有的PPO模型使用6B RM和 6B值函数。值函数1.3B和6B的学习率是9e-6，175B的学习率是5e-6；
预训练示例是 RL 训练集数的 8 倍，  预训练数据是从用于训练 GPT-3 模型的数据集中随机抽取的。
对于每个batch，我们在连续的步骤中计算 PPO 梯度和预训练梯度，并将它们都累积到梯度缓冲区中。
我们将预训练梯度乘以一个系数 γ = 27.8，以控制来自 PPO 和预训练分布的梯度的相对强度。

#### FLAN和T0模型
 我们通过在 FLAN 和 T0 数据集上微调 175B GPT-3 模型来获得我们的 FLAN 和 T0 基线。 将 T0 数据集下采样到 1M个数据点，以使每个模型的训练数据量具有可比性。
FLAN checkpoint：使用6B RM模型对prompt验证集进行评分。最后选择了4e-6 的学习率和 896k 个示例进行训练的checkpoint。
T0 checkpoiint。在一个实验中，我们使用了batch大小为128，学习率为4e-6，样本数为128万。另一个实验使用批大小为64，学习率为6e-6，样本数为100万。再次使用奖励模型分数，我们在学习率为4e-6 ,896k个训练样本后从前一个实验中选择了。


## 我的点评
自从chatgpt3大火之后，ai绘画也开始爆火。很多团队瞄准了这个方向去创业。这篇文章算是比较经典的文生图的范例，很值得细细研究阅读。

