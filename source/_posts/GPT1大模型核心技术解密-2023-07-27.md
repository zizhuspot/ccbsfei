---
title: GPT1大模型核心技术解密
date: 2023-07-27 23:30:00
categories:
  - 大模型
tags:
  - GPT1
  - GPT2
  - GPT3
  - 生成式预训练
description: 提出了一个框架，通过生成式预训练和判别式微调，让单一的任务无关模型具备强大的自然语言理解能力；利用无监督（预）训练来提高了判别任务性能；推动新的无监督学习研究
cover: https://cdn.jsdelivr.net/gh/1oscar/image_house@main/10024.png
---


## 重点

让我们把视角回到 2018 年，那个时候 NLP 在深度学习上基本还处于 word2vec 以及为不同任务做定制化深度模型的情况，虽然已经有 ELMo 这类预训练模型出现，但是其影响力还远远不足。在这个背景下，GPT 第一代预训练语言模型出现了。

GPT 原文标题为 Improving Language Understanding by Generative Pre-Training，即使用通用的预训练模型来提升语言理解能力（Generative Pre-Training 也可理解为“生成式预训练”）。GPT 这个名字就来源于 Generative Pre-Training。

从论文标题可以引出了两个问题：
1. 什么是通用？在学习通用的，迁移性强的文本特征表达时，什么目标函数是有效的？
2. 有了通用的特征表达之后，如何将它迁移到不同下游任务？
GPT 使用了预训练 + 微调的方式解决了这两个问题。

## GPT进化路线

![GPT进化路线](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/gpt%E8%BF%9B%E5%8C%96%E8%B7%AF%E7%BA%BF.jpg.png)

![GPT进化路线2](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/gpt%E8%BF%9B%E5%8C%96%E8%B7%AF%E7%BA%BF2.jpg)



## 摘要


因为标注数据少，文本蕴含、问答、语义相似度评估和文档分类等自然语言理解任务的辨别式训练模型难以表现良好 

在多样化的无标注文本语料库上对语言模型进行「生成式预训练」（即 GPT），然后对每个特定任务进行「判别式微调」，可以在这些任务上实现大幅能力提升


## 介绍

方向：有效地从原始文本中学习对于减轻自然语言处理（NLP）中对监督学习的依赖至关重要，实验证明有效。

挑战：
不清楚哪种类型的优化目标（optimization objectives）在学习对于迁移有用的文本表1（text representations）方面最有效；
如何最有效地将这些学到的表示迁移到目标任务上，尚无共识。

方案：
使用无监督预训练和监督微调相结合的半监督方法
采用了Transformer架构，其提供了更为结构化的记忆，以处理文本中的长期依赖性，实现了强大的迁移性能 。



## 相关工作

### NLP 的半监督学习

```
  统计信息
  词嵌入
  短语级
  句子级嵌入
```


### 无监督预训练

使用语言建模目标预训练神经网络（LSTM），然后在有监督的目标任务上对其进行微调

### Auxiliary training objectives 

添加辅助无监督训练目标是半监督学习的另一种形式
无监督预训练已经学会了与目标任务相关的多种语言方面


## 框架

### 无监督预训练
![GPT1框架](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/gpt1%E6%A1%86%E6%9E%B6.png)

预训练：输入含有大量token的语料库：U={U1,U2,…,Un}， GPT使用一个语言模型来极大化这个似然函数。具体的说，语言模型就是给定第i-k到第i-1个词，预测第i个词出现的概率。其中k被称为滑动窗口，当j的值被设置的很大的时候，模型将会看到更多的东西，当k的值被设计的很小时，模型将会看到更少的东西。

作者选择transformer的decoder作为骨干模型

当输入是U = (u−k, . . . , u−1) 时，将这些词通过映射矩阵转化为词嵌入，再加上位置嵌入，再通过transformer_block对其进行更新，最后输入到全连接层，得到最终的预测值。

其中，We为词嵌入矩阵，Wp为位置嵌入矩阵，ℎl为第l层 transformer 的输出，ℎn为最后一层 transformer 的输出，n为模型层数。

有别于基础transformer用的三角函数来做位置嵌入，该论文用的是可学习的位置矩阵来表征位置信息。在实际应用中，这两种方式似乎效果差别不大


![gpt1无监督预训练](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/gpt1%E6%97%A0%E7%9B%91%E7%9D%A3%E9%A2%84%E8%AE%AD%E7%BB%83.png)

分别例举了 NLP 中四个常见任务（文本分类、文本蕴含、文本相似度、问答任务）作为下游任务应用到 GPT 模型时，其输入序列是如何构造的，以及对应的预测层是如何设计的。
总的来说，都是通过在序列前后添加 Start 和 Extract 特殊标识符来表示开始和结束，序列之间添加必要的 Delim 标识符来表示分隔，当然实际使用时不会直接用 “Start/Extract/Delim” 这几个词，而是使用某些特殊符号。基于不同下游任务构造的输入序列，使用预训练的 GPT 模型进行特征编码，然后使用序列最后一个 token 的特征向量进行预测。
可以看到，不论下游任务的输入序列怎么变，最后的预测层怎么变，中间的特征抽取模块都是不变的，具有很好的迁移能力。


classification分类任务：一段文字，之后输出这段文字的标签。start+文本+extract输入到transformer_block中，之后将得到的结果输入到线性分类器中，得到最终的结果。
entailment蕴含任务：给出两段文字，之后输出这两段文字是否是相互关联的。start+文本1+delim+文本2+extract输入到transformer_block中，之后将得到的结果输入到线性分类器中，得到最终的结果。
similarity相似任务：给出两段文字，之后输出这两段文字是否相关。start+文本1+delim+文本2；start+文本2+delim+文本1，将以上两个标签输入到transformer_block中，之后将得到的结果输入到线性分类器中，得到最终的结果。
multiple choice多项选择任务：给出一个问题和几个候选选项，之后挑选出正确的答案。strat+文本+delim+候选选项1，……strat+文本+delim+候选选项n，分别输入到transformer_block中，之后将得到的结果输入到线性分类器中，得到最终的结果。


### 有监督fine-tuning

![gpt1有监督微调](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/gpt1%20%E6%9C%89%E7%9B%91%E7%9D%A3%E5%BE%AE%E8%B0%83.png)

在微调阶段，在有特定下游任务标签的情况下，给定输入序列X1到Xm，预测Y的概率，即将序列输入到预训练好的模型中，得到最后一层 transformer 的最后一个 token Xm的特征 Hlm，将得到的结果用softmax进行分类，得到最后的结果

微调阶段的目标函数为：

作者发现将预训练时的损失函数和有监督的损失函数加在一起，可以取得更好的效果，因此，作者将两部分损失加在一起，进行训练。

## 实验

### 启动 

![启动实验](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230727231126.png)

无监督训练 我们使用了BooksCorpus数据集[71]来训练语言模型。该数据集包含7000多本未发表的书籍，涵盖了各种流派，包括冒险、幻想和浪漫等。关键是，它包含了长串连续的文本，使生成模型能够学习到长距离信息的条件。另一个数据集是1B Word Benchmark，与ELMo [44]使用的数据集大小相当，但在句子级别上被洗牌，破坏了长距离结构。我们的语言模型在这个语料库上取得了非常低的令牌级困惑度，为18.4。

模型规格 我们的模型主要遵循了原始的Transformer工作[62]。我们训练了一个12层的只有解码器的Transformer，使用遮蔽自注意力头(768维状态和12个注意力头)。对于位置智能前向网络，我们使用了3072维内部状态。我们使用Adam优化方案[27]，最大学习率为2.5e-4。学习率在前2000个更新期间从零线性增加，并使用余弦计划降至0。我们以64个随机抽样、连续512个令牌的小批量进行100个时期的训练。由于整个模型中广泛使用了layernorm [2]，因此简单的权重初始化N(0，0.02)已经足够。我们使用了一个有40,000个合并的字节对编码(BPE)词汇表[53]，并对残差、嵌入和注意力丢失进行了0.1的正则化。我们还采用了[37]中提出的L2正则化的修改版本，所有非偏差或增益权重的w值为0.01。对于激活函数，我们使用了高斯误差线性单元(GELU)[18]。我们使用了学习的位置嵌入，而不是原始工作中提出的正弦版本。我们使用ftfy库[2]清理BooksCorpus中的原始文本，标准化一些标点符号和空格，并使用spaCy分词器[3]。

微调细节 除非特别说明，我们将超参数设置从无监督预训练中重复使用。我们在分类器中添加了0.1的dropout率。对于大多数任务，我们使用6.25e-5的学习率和32的batchsize。我们的模型快速进行微调，对于大多数情况，3次训练是足够的。我们使用线性学习率衰减计划，并在0.2％的训练时进行预热。λ设为0.5。


### 有监督微调

![有监督微调](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230727231215.png)

## 分析

![分析实验1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230727231330.png)

![分析实验2](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230727231418.png)

transformer层数的影响 GPT 的实验证明了模型的精度和泛化能力会随着解码器层数增加而不断提升，而且目前还有提升空间
我们观察了从无监督预训练到有监督目标任务中，转移不同层数的影响。图2(左)说明了我们的方法在多语言自然语言推理(MultiNLI)和RACE数据集上的表现随着转移层数的变化而变化。我们观察到转移嵌入向量可以提高性能，并且每个transformer层提供进一步的收益，完全转移在MultiNLI上提高了9%。这表明预训练模型中的每个层包含有助于解决目标任务的有用功能。

零样本行为 预训练模型具有 zero-shot 的能力，并且能随着预训练的进行不断增强
我们希望更好地理解transformer的语言模型预训练为什么有效。一个假设是底层生成模型为了提高其语言建模能力学会执行我们评估的许多任务，而越有结构的模型越能受益于这种学习，这使得模型能够零样本学习新任务。我们研究这个假设，我们在不经过任何微调的情况下检查了我们的模型在各种任务上的性能，以了解它的零样本能力。Transformer的注意力记忆机制有助于迁移学习，相比LSTM更加优越。有效性随着生成式预训练的进行而稳定增长，表明生成式预训练支持学习各种与任务相关的功能。我们还观察到，LSTM的零样本性能具有更高的方差，这表明Transformer体系结构的归纳偏差有助于迁移。 

消融研究 我们进行了三个不同的消融研究（表5）。首先，我们检查了没有辅助语言模型目标的情况下我们方法的性能。我们观察到辅助目标在 NLI 任务和 QQP 上有所帮助。总的趋势表明，大型数据集从辅助目标中受益，但小型数据集则没有。其次，我们通过将同样的框架与单层2048单元LSTM进行比较，分析了Transformer的效果。我们发现，当使用LSTM而不是Transformer时，平均得分下降了5.6分。LSTM仅在一个数据集MRPC上优于Transformer。最后，我们还直接比较了预训练前没有进行训练的Transformer架构与我们的完整模型在监督目标任务上的训练。我们观察到缺乏预训练会损害所有任务的表现，导致与我们的全模型相比下降了14.8％。

## 总结

```
提出了一个框架，通过生成式预训练和判别式微调，让单一的任务无关模型具备强大的自然语言理解能力
利用无监督（预）训练来提高了判别任务性能
推动新的无监督学习研究
```

## 我的点评

随着23年年初OpenAI公司chatgpt3大模型的发布，大模型这次是彻底火爆出圈了。几乎所有的互联网公司都在全民AI,我们公司年会制定的战略目标就是一整年的AI大模型。 这不，几乎所有的部门，不管是不是和这个相关的，都在往这个方向靠。

这是我写这篇文章的初衷。这仅仅是个开始，接下来，我会连续写十几篇都是相关的此类文章。

加油，你我！
