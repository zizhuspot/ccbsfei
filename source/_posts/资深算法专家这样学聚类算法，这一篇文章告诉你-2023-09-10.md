---
title: 资深算法专家这样学聚类算法，这一篇文章告诉你
date: 2023-09-10 12:40:00
categories:
  - 聚类
tags:
  - kmeans
description: 
cover: https://cdn.jsdelivr.net/gh/1oscar/image_house@main/10040.jpg
---

## 整体介绍

聚类算法里，有基于划分的算法，基于层次的算法，基于密度的算法，基于网格的算法，基于约束的算法。其中每一种基于的算法都会衍生出一至几种算法，对应的每一种算法不管在学术界还是工业界都存在着许多的改进的算法

## 基于划分算法

在基于划分的算法里，最典型的是K均值算法，这个比较简单，也是很容易理解的。

- 原理：
算法的输入：数据集合，K值
输出：经过聚类的Ｋ个中心点

```
1、在待聚类的数据集合里随机的选取Ｋ个数据样本点作为初始的聚类中心
2、计算剩余的每一个样本点距离Ｋ个聚类中心点的距离，分配样本点到最最近的聚类中心簇里
3、重新计算聚类簇的均值
4、判断聚类中心点是否会存在变化或者平方误差函数基本上不在变化（这个可以人为的设定，比如通过设定平方误差函数与上次相比满足一定的关系变停止算法等等），否则重复步骤2，3
算法优点
```


### 算法优点
很明显的是算法很容易，很简单，相对来说很快啊，仅限于小数据量。
### 算法的缺点
这个也是很明显的，根据算法原理，Ｋ值的人为性，主观性，要是有一个算法或者什么方式能够精确地确定Ｋ值变化给此算法带来突破；另外，初始聚类中心的随机性；还有数据集合肯定存在孤立点的影响以及大数据量的解决方案等等


### 改进思路
消除k的盲目性：   分层次的算法  或者  canopy算法

最小方差优化初始聚类中心：以方差最小的样本作为第一个簇中心，改进K-means++的初始选择第一个簇中心的随机性。

最大最小距离法：选择距离样本中心距离最小的点中最大的值。
最大最小距离算法用于避免初始聚类中心分布过于集中和密集，保证聚类中心点之间的距离相差较
远，能够充分的体现数据集的整体分布情况


## 评估指标

轮廓系数（Silhouette Coefficient）、Calinski-Harabasz指数（简称CH指数）和Davies-Bouldin指数（简称DB指数）是常用的聚类效果评估指标。
它们分别从不同的角度来评估聚类的质量。

### 轮廓系数

轮廓系数更注重个体与所属簇内其他样本的相似性以及与最近的其他簇的簇中心的相似性。当数据集中的簇内差异较大或者簇间差异较大时，轮廓系数可能会有较好的表现。


是基于样本之间的相似度和样本与所属簇中心的相似度来计算的,轮廓系数的值在-1到1之间，越接近1说明聚类效果越好。如果轮廓系数为0，则表示样本在两个簇中的分布比例相等，无法明确应该将其归类到哪个簇。

轮廓系数 = (b - a)/max(a, b)

    如果一个数据点完全位于自己的簇内，并且与其他簇的中心点距离较远，则该数据点的轮廓系数为 1。
    如果一个数据点位于两个或多个簇的边界上，则该数据点的轮廓系数接近 0。
    如果一个数据点位于其他簇的内部，则该数据点的轮廓系数接近-1。

计算方法如下：

a. 对于每个点i，计算它与同簇其他点的平均距离，得到a(i)；

b. 对于每个点i，计算它与不同簇最近点的平均距离，得到b(i)；

c. 计算轮廓系数，公式为：s(i) = (b(i) - a(i)) / max(a(i), b(i))；

d. 求出所有点的轮廓系数的平均值，即为最终的轮廓系数。


如果是pyspark，在 PySpark 中，你可以使用 ClusteringEvaluator 来计算轮廓系数。通过比较不同 k 值对应的轮廓系数，选择使得轮廓系数最大的 k 值。


### Calinski-Harabasz指数（CH指数）：

CH指数更注重簇内部的凝聚程度和簇间的分离程度。当数据集中的簇内差异较小时，CH指数可能会有较好的表现。


CH指数主要用于评估多个簇的情况

CH 指数和 DB 指数的取值范围为[0,1]

CH指数 = (∑(w_i - w_j)^2) / (∑w_i^2 * ∑w_j^2)
其中，w_i 和 w_j 分别是第 i 个簇和第 j 个簇的权重，即每个簇内的样本数量。CH指数的值范围在0到1之间，值越大表示聚类效果越好。


    如果所有簇之间的分离程度都很高，则 CH 指数和 DB 指数接近 1。
    如果不同簇之间存在重叠或混杂，则 CH 指数和 DB 指数接近 0。


CH指数越大，说明聚类效果越好。计算方法如下：

a. 计算类间协方差矩阵和类内协方差矩阵；

b. 计算两个协方差矩阵的特征向量和特征值；

c. 根据特征值计算类间散度矩阵和类内散度矩阵；

d. 计算Calinski-Harabasz指数，公式为：CH(K) = trace(W)/(n-1) * trace(B)/(n-1) / (||W|| ||B||/(n-1)^2)，其中W和B分别是类内散度矩阵和类间散度矩阵，n是数据点的数量，trace表示求矩阵的迹，|| ||表示求矩阵的Frobenius范数；

e. 选择使CH指数最大的K值作为最佳聚类数。

###  Davies-Bouldin指数（DB指数）：

DB指数更注重簇的大小和簇内的紧密程度。当数据集中的簇大小相近且簇内紧密程度较高时，DB指数可能会有较好的表现。


DB指数同样主要用于评估多个簇的情况

DB指数越小，说明聚类效果越好。

DB指数 = ∑(w_i * d_i) / (∑w_i + ∑d_i)
其中，w_i 是第 i 个簇的权重，即每个簇内的样本数量;d_i 是第 i 个簇内所有样本之间的平均距离。DB指数的值越小表示聚类效果越好。

计算方法如下：

a. 对于每个簇Ck，计算簇内最大簇间距离中西尔勒斯平均值的最大值，记为马克v恩（m(Ck)）；

b. 对于每个簇Ck和Cj，计算簇间距离中西尔勒斯平均值的最大值，记为σj；

c. 计算Davies-Bouldin指数，公式为：DB(K) = (sum(m(Ck)) + sum(σj)) / K；

d. 选择使DB指数最小的K值作为最佳聚类数。

这些指标可以用来评估K-means聚类的效果，选择合适的K值。在实际应用中，通常会尝试不同的K值，并使用这些指标进行评估和比较，以选择最佳的聚类结果。


## 应用场景

- 用户画像-客户价值分类

客户价值分类是客群分析的重要组成，采用电商RFM模型，抽样进行聚类中心的计算，通过计算客户与聚类中心的距离，判断客户价值。

- app聚类


- 用户地理位置聚类 

- 用户分群 

- 推荐

改进算法融合的性能方面的一个经验：如果有足够的数据，在协同过滤算法之前，先用聚类，这样可以缩短协同过滤算法在相关邻居上的选择问题上的时间；一个聚类就是一个属于这个聚类的偏好

- IP聚类

通过用户的来源IP聚类分析来判断是否有地域的汇聚，从而发现一些和地域相关的问题，类似的做法可以把种种隐藏在监控数值下的一些非显性问题暴露出来。


## 应用场景举例

### 用户路径的聚类

``` 
用户路径的聚类可以通过一系列算法实现，其中包括K-Means聚类、层次聚类等。以下是一种可能的使用K-Means聚类进行用户路径分析的步骤：

数据准备：收集并清洗广告用户路径数据，包括用户与广告的交互行为，如点击、浏览、购买等。
数据预处理：对数据进行标准化或归一化处理，消除数据量纲的影响。
构建序列：将广告用户路径数据转换为序列形式，每个序列包含了一组用户与广告的交互行为。
序列标注：对每个序列进行标注，以便于后续的聚类分析。标注可以基于用户的属性、行为等进行分类。
序列聚类：使用适合序列数据的聚类算法进行聚类分析。例如，可以考虑使用K-Means聚类、层次聚类、DBSCAN聚类等。
结果分析：对聚类结果进行解读，分析不同类别的广告用户路径序列的特点，以便于进行更有针对性的广告推送。

```

### 代码实现

#### 纯python实现 

- 解释：
首先使用 NumPy 库生成了 100 个随机的二维数据点。然后，我们使用 K-Means 算法对这些数据进行聚类。在算法执行过程中，我们记录了每个点到聚类中心的距离，并使用标签记录了每个点的聚类结果。最后，我们使用 Matplotlib 库可视化了聚类结果。

需要注意的是，在实际应用中，我们可能需要根据具体的需求对 K-Means 算法进行调整和优化。例如，我们可以使用不同的距离度量方法、初始化方法、迭代次数等参数来提高聚类效果。

``` python
import numpy as np  
import matplotlib.pyplot as plt  
  
# 生成随机数据  
np.random.seed(0)  
X = np.random.randn(100, 2)  
  
# 初始化聚类中心  
n_clusters = 3  
centers = np.zeros((n_clusters, 2))  
centers[0] = np.array([-4, -2])  
centers[1] = np.array([0, 2])  
centers[2] = np.array([4, -2])  
  
# 执行 K-Means 算法  
iterations = 100  
distances = np.zeros((iterations, X.shape[0]))  
labels = np.zeros((iterations, X.shape[0]))  
for i in range(iterations):  
    # 计算每个点到聚类中心的距离  
    for j in range(X.shape[0]):  
        distances[i, j] = np.linalg.norm(X[j] - centers)  
    # 对每个点进行聚类，并记录标签  
    labels[i] = np.argmin(distances[i], axis=0) + 1  
    # 更新聚类中心  
    for j in range(n_clusters):  
        centers[j] = np.mean(X[labels == j], axis=0)  
  
# 可视化聚类结果  
colors = ['r', 'g', 'b']  
for i in range(n_clusters):  
    plt.scatter(X[:, 0], X[:, 1], c=colors[i % 3], label=i)  
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)  
plt.legend()  
plt.show()

```

#### sklearn模块实现

- 解释：

首先从数据中提取了特征并对其进行了标准化。然后，我们创建了一个新的数据帧，其中包含经过标准化的数据。接着，我们使用DataFrameMapper将数据映射到适当的格式，以便将其输入到KMeans模型中。最后，我们得到了预测结果，并将它们添加回到原始数据中。

``` python


from sklearn_pandas import DataFrameMapper
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# 假设你的数据是一个DataFrame，包含两列：features和labels
data = sqlContext.createDataFrame(<your data>)

# 特征缩放
scaler = StandardScaler()
scaled_data = scaler.fit_transform(data['features'])

# 将数据转换为DataFrame
scaled_rows = list(zip(*numpy.c_[list(range(len(data))), scaled_data]))
schema = StructType([StructField('id', IntegerType(), False), \
                     StructField('features', FloatType(), False)])
scaled_dataframe = sqlContext.createDataFrame(scaled_rows, schema)

# 将数据映射到适当的格式
mapper = DataFrameMapper([(\'features\', Scaling(StandardScaler()))])
normalized_dataframe = mapper.fit_transform(scaled_dataframe)

# 运行K-Means算法
kmeans = KMeans(k=2, random_state=0)
predictions = kmeans.fit_predict(normalized_dataframe)

# 将预测结果添加回原始数据
data['prediction'] = predictions

# 显示前几行
print(data.head())

```



#### pyspark实现版本1 

- 解释

假设你的数据是以 LIBSVM 格式存储的。如果你的数据是不同的格式，你需要相应地调整数据读取部分。此外，你需要将 "path_to_your_data" 替换为你的实际数据路径。

在这个例子中，我们使用了 K-Means 算法对数据进行聚类，然后评估了聚类结果的质量。最后，我们打印了每个簇的聚类中心。

```python

from pyspark.sql import SparkSession  
from pyspark.ml.clustering import KMeans  
from pyspark.ml.evaluation import ClusteringEvaluator  
  
# 创建 SparkSession  
spark = SparkSession.builder.appName('KMeans Example').getOrCreate()  
  
# 加载数据集  
dataset = spark.read.format("libsvm").load("path_to_your_data")  
  
# 设置 KMeans 算法参数  
kmeans = KMeans(k=3, seed=1, featuresCol="features")  
  
# 训练模型  
model = kmeans.fit(dataset)  
  
# 预测  
predictions = model.transform(dataset)  
  
# 评估聚类模型 (使用 Silhouette Score)  
evaluator = ClusteringEvaluator()  
  
silhouette = evaluator.evaluate(predictions)  
print("Silhouette with squared euclidean distance = " + str(silhouette))  
  
# 显示聚类中心  
centers = model.clusterCenters()  
print("Cluster Centers: ")  
for center in centers:  
    print(center)
```

#### pyspark实现版本2

- 解释

使用了 iris.txt 数据集，其中有 3 个类别，每个类别有 50 个样本。我们使用 KMeans 算法将这 3 个类别分为 3 个簇。在训练模型时，我们使用了 nClusters=3 和 randomSeed=42 参数，其中 nClusters 表示簇的数量，randomSeed 表示随机数生成器的种子。最后，我们使用 predict() 方法对 DataFrame 进行预测，并显示预测结果。


```python
from pyspark.sql.functions import *
from pyspark.ml.clustering import KMeans

# 创建 SparkSession
spark = SparkSession.builder.appName("KMeans").enableHiveSupport().getOrCreate()

# 创建 DataFrame
df = spark.sql("SELECT * FROM iris.txt")

# 创建 KMeans 模型
kmeans = KMeans(nClusters=3, randomSeed=42)

# 训练模型
model = kmeans.fit(df)

# 进行预测
prediction = model.predict(df)

# 查看结果
prediction.show()

```

#### pyspark实现版本3

- 解释

首先创建了一个包含四个样例的DataFrame，每个样例都有一个特征'features'和一个目标变量'label'。然后，我们将特征'features'转换为数值型，接着创建了一个KMeans模型实例，并使用训练好的模型对原始数据进行预测。最后，我们打印出了预测结果和每个样例所属的簇。


```python
from pyspark.ml.clustering import KMeans
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.linalg import DenseVector
from pyspark.sql import DataFrame
from pyspark.sql.functions import col

# 假设我们有一个包含特征'features'和目标变量'label'的DataFrame
df = spark.createDataFrame([(DenseVector([1.0, 2.0]), "A"),
                          (DenseVector([3.0, 4.0]), "B"),
                          (DenseVector([5.0, 6.0]), "A"),
                          (DenseVector([7.0, 8.0]), "B")], ["features", "label"])

# 将特征'features'转换为数值型
vectorAssembler = VectorAssembler(inputCols=["features"], outputCol="featuresNum")
df = vectorAssembler.transform(df)

# 创建KMeans模型实例
kmeans = KMeans(k=2, seed=1, featuresCol="featuresNum", labelCol="label")

# 训练模型
kmeansModel = kmeans.fit(df)

# 预测
predictions = kmeansModel.transform(df)

# 打印预测结果
print("Cluster centers:")
display(predictions.select("featuresNum", "prediction"))

# 打印每个样本所属的簇
print("Sample points and their cluster assignments:")
for i in range(len(predictions)):
    row = predictions.head(i+1).collect()[0]
    print(f"Sample {i + 1}: {row['featuresNum']}, Cluster={row['prediction']}")

```


#### pyspark实现版本4 含选择最佳簇个数代码 

- 解释

首先创建了一个包含稀疏向量和标签的RDD。然后，我们将这些数据转换成特征和标签，以便输入到KMeans模型中。接下来，我们使用VectorAssembler将特征组合在一起，并将标签转换成数字。最后，我们使用KMeans模型进行训练，并对模型进行评估。


```python
from pyspark.ml.clustering import KMeans
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.evaluation import ClusteringEvaluator

# 准备数据
data = [(Vectors.sparse(5, {0: 4.0, 1: 7.0}), "a"), (-2.0, -3.0, 6.0, 8.0, 9.0, "b")]
dataset = sc.parallelize(data)
rdd = dataset.map(tuple)

# 特征组合
vector_input = rdd.map(lambda row: tuple([row[0].toArray(), ]))
label_input = rdd.map(lambda row: int(row[1]))
combinedInput = vector_input.zip(label_input)

# 特征预处理
assembler = VectorAssembler(
    inputCols=["features"], outputCol="features")
output = assembler.transform(combinedInput)

# 训练模型
numClusters = 2
kmeans = KMeans(k=numClusters, seed=1, featuresCol='features', labelCol='label')
model = kmeans.fit(output)

# 评估模型
evaluator = ClusteringEvaluator(
    metricName="silhouette", featuresCol="features", predictionCol="prediction")
silhouetteScore = evaluator.evaluate(model.transform(output))
print("Silhouette with squared euclidean distance: %f" % silhouetteScore)

```





#### pyspark 评估指标代码

##### 轮廓系数（Silhouette Score） 

```python


from pyspark.ml.evaluation import ClusteringEvaluator  
  
# 假设 dataset 是你的数据集，k_values 是可能的 k 值集合  
for k in k_values:  
    kmeans = KMeans(k=k, seed=1, featuresCol="features").fit(dataset)  
    predictions = kmeans.transform(dataset)  
    silhouette = ClusteringEvaluator().evaluate(predictions)  
    print("Silhouette with k = {}: {}".format(k, silhouette))

```

##### 肘部法则（Elbow Method）：

通过计算不同 k 值对应的簇内误差平方和（Within-Cluster Sum of Squares，WCSS），绘制 WCSS 随 k 值变化的曲线图。选择使得曲线图“肘部”凹陷处的 k 值，即 WCSS 显著下降的转折点

```python

from pyspark.ml.clustering import KMeans  
from pyspark.sql import functions as F  
  
# 计算 WCSS  
wcss = dataset.groupBy(F.expr("prediction")).agg(F.pow(F.col("raw_distance"), 2).alias("wcss"))  
  
# 绘制肘部图  
k_values = [2, 3, 4, 5, 6]  # 尝试不同的 k 值  
wcss_by_k = [wcss_df.groupBy("k").agg(F.sum("wcss").alias("wcss")).collect()[0]["wcss"] for wcss_df in [wcss]]  
k_values_with_wcss = [(k, wcss) for k, wcss in zip(k_values, wcss_by_k)]  
max_wcss_k = max(k_values_with_wcss, key=lambda x: x[1])[0]

```

##### 交叉验证（Cross-Validation）：

对于较大的数据集或需要更精确的选择时，可以使用交叉验证方法。PySpark 的 KMeansModel 提供了一个 computeCost 方法，可以用于计算点到簇中心的距离之和，这可以作为交叉验证的指标。

##### 桑基图

聚类结果通常可以用热力图、散点图、树状图等多种形式进行可视化分析。但是，对于大型数据集，特别是当聚类数量较多时，传统的可视化方法可能难以有效地展示所有信息。这时，桑基图就是一个很好的选择。

桑基图能够以分层的方式展示数据的流动和转换过程，这与聚类的思想非常契合。同时，由于其对数据规模不敏感的特点，即使是非常大规模的数据集，也能够得到有效的展示。


Python中实现桑基图的一种常见方法是使用NetworkX库。

- 解释

首先定义了一个简单的网络图，然后使用nx.maximum_flow函数计算了最大流量。接着，我们使用nx.draw函数绘制了桑基图，最后使用plt.show()显示了图像。


```python

import matplotlib.pyplot as plt
import networkx as nx

# 定义节点和边
nodes = ['A', 'B', 'C', 'D', 'E']
edges = [('A', 'B'), ('A', 'C'), ('B', 'D'), ('C', 'E')]

# 创建网络图
G = nx.DiGraph()
G.add_nodes_from(nodes)
G.add_edges_from(edges)

# 计算流量
flow = nx.maximum_flow(G, 'A', 'E')

# 绘制桑基图
plt.figure(figsize=(10, DCenter)) # DCenter是图的中心点
pos = nx.spring_layout(G)
nx.draw(G, pos, with_labels=True, node_color='skyblue', edge_color='black', width=1.5)
nx.draw_networkx_edge_labels(G, pos, font_size=10, labels={e: f'{f}' for e, f in flow.items()})

# 添加标题
plt.title('Sankey Diagram')

# 显示图像
plt.show()

```


## 相关聚类代码汇总-持续更新 

- 大数据集下的dbscan聚类  : 实现了并行dbscan 。 
https://github.com/lyonling/SparkDBSCAN/tree/cacfcfbb5207fe8a74ad258b11bba08d1d75aaee


- 深度学习聚类 facebook pytorch实现
https://github.com/facebookresearch/deepcluster


- 深度学习聚类 torch 
https://github.com/asanakoy/deep_clustering

- 深度学习聚类 torch
https://github.com/xiaopeng-liao/DEC_pytorch 可读   

- 聚类实现  TensorFlow  
https://github.com/alex-sage/logo-gen  
