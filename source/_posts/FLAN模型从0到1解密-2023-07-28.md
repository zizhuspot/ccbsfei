---
title: FLAN模型从0到1解密
date: 2023-07-28 10:30:00
categories:
  - 大模型
tags:
  - GPT1
  - GPT2
  - GPT3
  - 生成式预训练
  - fine-tuning
  - zero-shot 
description: 探讨零样本提示的一个简单问题,FLAN模型相对于未做微调的模型提高了性能，大多数任务上超过零样本GPT-3
cover: https://cdn.jsdelivr.net/gh/1oscar/image_house@main/FLAN.png
---


## 简介：Abstract

### 问题：零样本学习
大规模语言模型（比如GPT-3）在小样本学习上性能很好，在零样本学习上不够好
在阅读理解、问答、语言推理等任务上，GPT-3的零样本性能比小样本差很多
可能的原因：（对于零样本的陌生任务），没有了小样本的范例，输入提示语的格式与预训练数据不同，模型难以有好的性能
探索一种简单的方法，提高零样本性能

### 方法：指令微调（instruction tuning)
利用直觉：NLP任务可以用自然语言指令来描述
137B参数预训练模型，
60多个NLP数据集，用自然语言指令模板表述
用陌生任务数据集评估零样本性能。数据集按照任务类型分组。评估某一组的时候，用其他组作为训练集。
指令微调后，命名为FLAN - Finetuned Language Net

### 自然语言指令模板

![自然语言指令模板](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230728225751.png)

**结果：大幅提升了在陌生任务上的零样本性能**

大幅高于137B参数预训练模型，

20/25个数据集高于零样本157B参数GPT-3，

在一些数据集高于小样本GPT-3（ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze）

消融实验表明，指令微调成功的关键是：数据集数量，模型规模，自然语言指令

![对比实验](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230728225832.png)

**指令微调是一种简单的方法，结合了预训练-微调、prompting范式的优点**

预训练-微调：每个任务一个模型，需要任务特定的例子

prompting：加入小样本示例

    GPT1：无监督预训练 → 下游任务微调
    GPT2：把下游任务作为条件放到预训练中 P(output|input, task)
    GPT3：预训练 → 给出下游任务的例子prompt（小样本）或者不给（零样本），不需要微调
指令微调：预训练 → 在很多任务上指令微调 → 在陌生任务上推理

![指令微调](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230728230027.png)



## 方案：

### FLAN：指令微调改善零样本学习
思路：改善模型对自然语言指令的响应能力 
用监督学习、指令描述，教语言模型处理任务。模型学习遵从指令，适用于陌生任务 
数据集按照任务类型分组。选一组用于评估，其他组用于微调训练。 

### 任务和模板
从头创建许多任务的指令微调数据集是困难的。所以，把研究社区已有的数据集转换为指令格式。
Tensorflow Datasets上的62个文本数据集，包括语言理解和生成任务，汇总为一个合集 

    对每个数据集，手动创建10个模板，用自然语言指令描述数据集的任务
    为了增加多样性，其中3个模板带有误导性 
    例如，对情感分类任务，用模板请求生成一篇影评 
    在所有数据集的合集上，指令微调一个预训练模型 
    对每条数据随机选一个该数据集的指令模板 

![方案1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230728230159.png)


### 评估拆分
需要定义什么是陌生任务
过去的工作按照数据集划分，不在训练集的数据集就是陌生的	

我们使用更严格的定义，按照任务类型划分（图表3），只有数据集所属的任务类型不在训练集中、才是陌生的	
为了评估c个任务分组的零样本性能，需要指令微调c个模型，每个用对应的一组数据集评估	
备注：常识阅读理解 这一组 与 常识、阅读理解 这两组互斥，自然语言推理（NLI）和复述（paraphrase）互斥

![评估拆分](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230728230437.png)


我们用选项后缀来限制表达方法，把OPTIONS作为token加到分类任务的末尾、伴随一个类别列表（图表1）

### 训练细节：模型结构、预训练

    模型：LaMDA-PT, a dense left-to-right, decoder-only transformer language model of 137B parameters (Thoppilan et al., 2022). 
    预训练数据：网络文档（包括代码）、对话、维基百科
    文本拆分成2.49T的BPE token，词汇量32k，分词器使用 SentencePiece library (Kudo & Richardson, 2018)

    BPE(Byte-Pair Encoding)
    分词，统计词频	
    ("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
    分解单词得到字符，去重，作为词汇表	
    [“b”、“g”、“h”、“n”、“p”、“s”、“u”]
    相邻两个符号组成一对，把高频的符号对合并为一个新的符号，加入词汇表	
    u和g组成ug
    ("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)
    重复上一步，直到词汇量达到阈值、或者最高频的频次=1	
    经过三轮合并后的词汇表 ：[ “b”、“g”、“h”、“n”、“p”、“s”、“u”、“ug”、“un”、“hug” ]

#### 训练细节：指令微调过程，基于LaMDA-PT模型
混合所有数据集，随机采样
为了平衡不同规模的数据集
每个数据集的训练样本数量限制为30k
遵守样本比例混合方案 examples-proportional mixing scheme (Raffel et al., 2020)
最大混合率为3k（3000个以外的样本不会有额外的采样权重）

梯度步长（gradient steps）： 30k
**batch size：** 8192 个token

**优化器：** Adafactor Optimizer (Shazeer & Stern, 2018)

**学习率：** 3e-5

**输入序列长度：** 1024

**目标序列长度：** 256

使用打包 packing (Raffel et al., 2020) 把多个训练样本合并到一个序列中
用一个特殊的EOS token分隔输入和目标

**硬件：** TPUv3， 128核

**微调训练时间** 大约60小时

评估使用的模型（checkpoint）是经过30k steps的最终结果



## 结果：

在多个任务上评估，如2.2所述。评估每个任务的模型使用不同分组的训练集
对每个数据集，评估所有模板的平均性能
手动进行提示语工程，得到开发集 a dev set is sometimes available for manual prompt engineering (Brown et al., 2020)
对每个数据集，用开发集上的最佳模板评估了测试集的性能

对照组，零样本和小样本LaMDA-PT，与GPT-3相同的提示语（LaMDA-PT如果没有指令调优，不适用于自然的指令）
在大多数数据集上，指令微调对模型有显著提升

也对比了零样本的GPT-3 175B (Brown et al., 2020) 和 GLaM 64B/64E (Du et al., 2021)，使用论文的数据
使用最佳开发集的模板，
25个数据集：零样本FLAN在20个上胜过零样本GPT-3，在10个上胜过小样本GPT-3 
19个数据集：零样本FLAN在13个上胜过零样本GLaM，在11个上胜过one-shot GLaM

![结果](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230728230749.png)

**总结，指令微调对于可以自然描述为指令的任务是很有效的**

例如，自然语言推理NLI，问答QA，翻译，结构转文本

**对于直接标注为语言建模的任务，效果较差。指令本身往往是多余的**

例如，常识推理、共指消解任务，格式是补全一个不完整的句子或段落

自然语言推理 NLI：5个数据集，模型根据一些前提、判断假设是否为真。
FLAN大幅超过其他baselines
参考  Brown et al. (2020)，GPT-3难于处理NLI的可能原因是，NLI样本较少出现在无监督的训练集里，因此很难被表述为一个句子的延续
对于FLAN，把NLI任务表述为更自然的 “Does <premise> mean that <hypothesis>?” 获得了更高的性能
阅读理解：模型回答关于一个段落的问题
闭卷问答：模型回答问题，不能访问包含答案的信息
翻译：与GPT-3类似，训练集约90%是英语，包含其他语言文本（不专门用于训练翻译模型），翻译成英语效果较好，因为使用的是英语句子分词器，大部分预训练数据是英语的
其他任务：指令调优的一个局限性是，没有提高许多语言建模任务的性能
例如，常识推理，共指消解，表述为句子补全的形式（附录的表2），FLAN只在3项优于LaMDA-PT，这意味着，如果下游任务与预训练的目标相同（指令很大程度是多余的），指令微调是没用的



## 消融实验
### 指令调优的（任务）分组数量

NLI、闭卷问答、常识推理这3个组用于评估，其他7个组用于微调训练
备注：没有用复述、常识阅读理解，因为他们与NLI和常识推理过于相似 
图表6是结果，横轴：训练使用1~7个组，纵轴：评估使用3个组 
加入训练的组越多，评估的性能越好（情感分析这一组除外）
从趋势上看，7个组对性能的提升没有达到饱和，如果加入更多的组，模型性能可能进一步提高
注意：这个实验不能得出哪个组贡献最大的结论（即使情感分析的附加值最小）

![消融实验1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230728230928.png)

### 规模定律
Brown et al. (2020) 表明大模型对语言模型的零样本、小样本能力有提升，
我们探索模型规模对指令调优的影响
使用与4.1 相同的任务分组，评估模型参数量为：422M、2B、8B、68B 和 137B，如图表7所示
对于100B参数附近的两个模型，指令调优显著提升性能
对于8B或更小的模型，指令调优反而降低了模型性能
可能的解释，小模型在学习约40个任务时，模型容量会不够用，导致在新任务上表现更差
基于这个解释，对于大模型，指令调优虽然占用了一部分模型容量，但也教会模型如何遵从指令，使他们用剩余容量泛化到新任务

![规模定律1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230728231008.png)

### 指令的作用
排除一种可能：模型提升完全来自多任务微调，即使没有指令也可以。
考虑两种没有指令的微调设置
无模板设置
例如，翻译任务，输入：The dog runs，输出：Le chien court
数据集名称设置
例如，翻译成法语，输入：“[Translation: WMT’14 to French] The dog runs.”
这两种设置与FLAN的微调进行对照
FLAN使用自然的指令，例如，“Please translate this sentence to French: ‘The dog runs.’”
用图表5的4个组进行评估（自然语言推理、阅读理解、闭卷QA和翻译
对于无模板设置，在零样本推理时使用FLAN的指令（如果不用模板，模型不知道要执行什么任务）
对于数据集名称设置，使用名称和FLAN指令。
如图表8所示，两种消融配置比FLAN差很多，说明指令对于陌生任务的零样本学习至关重要


## 相关工作

研究涉及了很多领域：零样本，提示，多任务学习，NLP应用语言模型（附录D）
这里重点描述两个关联最密切的子领域

我们要求模型响应指令的方式，类似基于问答的任务(Kumar et al., 2016; McCann et al., 2018)
为了统一NLP任务，把各种任务转换为基于上下文的问答
虽然非常相似，但他们侧重于多任务学习、而非零样本学习
他们没有考虑在预训练模型中使用现有知识，Liu et al. (2021)
我们的工作在模型规模、任务范围上超过了近期的工作 Chai et al. (2020) and Zhong et al. (2021)

语言模型的成功引发了对模型遵从指令能力的初步研究
近期，Mishra et al. (2021) 在小样本示例和指令数据上微调了140M参数的BART，评估在陌生任务上的小样本能力，类似我们在4.4的小样本指令微调
Ye et al. (2021)，没那么强调指令
这些结果表明，在多个任务上微调可以提高陌生任务的小样本性能，即便是小模型

Sanh et al. (2021) 微调训练T5，设置与我们类似，发现零样本学习在11B参数的模型中可以改进
OpenAI的InstructGPT模型规模与我们类似，通过微调和强化学习训练，产生人类评估者偏好的输出 Ouyang et al., 2022



## 讨论

论文探讨零样本提示的一个简单问题：在表述为指令的任务上微调模型，能否提高陌生任务上的性能
用指令微调来解决问题，结合了预训练-微调和提示范式的优点
FLAN模型相对于未做微调的模型提高了性能，大多数任务上超过零样本GPT-3
消融实验表明，任务分组的数量可以提高陌生任务的性能。指令微调的提升只适用于规模足够大的模型
指令微调可以与其他提示方法结合，比如小样本提示、提示微调
大规模语言模型的各种能力引起了人们对专家模型和通才模型的关注，我们的研究有潜在意义
人们可能认为标注数据对于提高专业模型有作用。指令微调展示了如何用标注数据帮模型执行许多陌生的任务。
指令微调对跨任务泛化的提升表明，特定任务训练是通用语言模型的补充，激发了对通才模型的进一步研究

我们研究的局限性
任务分组有一定的主观性（尽管我们尝试使用文献公认的分类）
我们只研究了通常为一个句子的较短指令
个别样本可能出现在预训练数据中，包括网络文档，尽管在附录C的事后分析没有发现数据重叠对结果产生重大影响
FLAN 137B的规模使得服务成本很高

未来工作
收集/生成更多的任务分组，用于微调、跨语言实验
使用FLAN生成数据，用于训练下游的分类器
用微调改进模型行为的偏差和公平性 bias and fairness (Solaiman & Dennison, 2021).

## 结语

本文总结
道德
标注数据集可能包含不良的偏差，偏差可能传播到下游的零样本应用中
指令微调可能减少了对数据和专业知识的需求，降低准入门槛可能带来好处和风险
环保
预训练模型与Austin et al. (2021)相同，耗能是 451 MWh，碳足迹是 26 tCO2e
微调FLAN增加的steps与预训练相比不到2%，因此额外的能量成本很小

## 附录

附录B 更多的消融实验
	任务组的数量不变、增加数据集数量（每组1~4个数据集），每个数据集用1、4、10个模板
	增加数据集数量，有明显提升
	模板数量的影响不明显，这对于设计模板的初衷是个打击（希望用10个模板泛化到任意的模板上）
		猜测：大规模模型不容易在特定任务上泛化

附录C 数据污染分析
	预训练数据有超过2T个token，担心其中包含了评估数据集的某些数据，会影响零样本的评估
	与GPT-3类似，对数据污染进行事后分析
		遵循 Brown et al. 2020，加工一个“干净”的评估数据集：计算 n-gram 覆盖，剔除可能出现在预训练集的样本
	横轴：每个数据集的干净样本比例，纵轴：数据清洗前后的性能变化


## 我的点评
本篇只能算是大模型演进中的一个插曲环节。本身没有翻起太大的风浪。

