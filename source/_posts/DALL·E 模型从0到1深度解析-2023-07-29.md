---
title: DALL·E 模型从0到1深度解析
date: 2023-07-29 10:21:00
categories:
  - AI绘画
tags:
  - 文生图
  - ai画图
  - clip模型
  - diffusion model
  - DALL.E 
description: 训练一个 120 亿参数的自回归transformer，使用互联网收集的2.5亿个图像文本对，得到一个可用自然语言控制的、灵活、高保真的图像生成模型

---

## 简介：

### 问题：文本生成图像(text-to-image）
- 以往关注的是在固定数据集上建模：结构，损失函数，辅助信息（标签或mask）等等
    - Mansimov et al. (2015)：证明DRAW Gregor et al. (2015) 的生成模型可以应用到图像描述- 上（image captions）
    - Reed et al. (2016b)：使用 Goodfellow et al., 2014 的生成对抗式网络，而不是循环变分- 自动编码器（RVAE），可以提高图像保真度。不仅可以生成已知的物体，还可以零样本泛化到其他- 类别。
    - 修改多尺度的生成器(Zhang et al., 2017; 2018)
    - 整合了注意力和辅助损失(Xu et al., 2018)
    - 利用文本以外的其他信息(Reed et al., 2016a; Li et al., 2019; Koh et al., 2021)
    - Nguyen et al. (2017) 基于能量的图像生成框架。可以结合预训练的判别模型，比如MS-COCO数- 据集上预训练的图像描述模型
    - Cho et al. (2020) 优化对预训练的跨模态掩码语言模型的输入。

- 虽然视觉保真度（visual fidelity）显著提高，仍然有一些严重问题：对象失真，对象放置不符- 合逻辑，前景背景的不自然混合

- 大规模生成模型的进展可能提供了一个改进方向


### 大规模生成模型的进展可能提供了一个改进方向

- 自回归transformer (Vaswani et al., 2017)(Attention is All you Need) 在多个领域取得了令人瞩目的成果：文本 (Radford et al., 2019)、图像 (Chen et al., 2020）、音频（Dhariwal et al., 2020）。
- 相比之下，文本到图像的生成通常在相对较小的数据集上进行评估，例如 MS-COCO 和 CUB-200（Welinder 等人，2010）。
- 数据集大小和模型大小是否会成为文本生成图像任务的限制因素？

**方法：**

我们训练一个 120 亿参数的自回归transformer，使用互联网收集的2.5亿个图像文本对，得到一个可用自然语言控制的、灵活、高保真的图像生成模型

**结果：**

在MS-COCO数据集上零样本的评估，实现了高质量的图像生成
人工评估，有90%的结果优于以往在数据集上训练的模型
能够执行复杂的任务，比如图像到图像的转换

### 生成图像效果
![生成图像效果1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729102732.png)

![生成图像效果2](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729102756.png)


## 方案：

**目标是训练一个transformer(Vaswani et al., 2017)，把文本和图像token建模成一个数据流**
- 如果把每个像素作为token，高分辨率的图像会占用大量内存
- (Salimans et al., 2017) 似然度目标(likelihood objectives)优先对像素之间的短距离依赖性建模，模型容量大部分用于捕捉高频细节、而不是我们视觉识别的低频结构

为了解决这些问题，我们使用两阶段的训练流程(Oord et al., 2017; Razavi et al., 2019)

- 阶段1：训练一个离散变分自动编码器 (dVAE)
    - 将每个 256×256 RGB 图像压缩成 32×32个token的网格，每个token可以假设 8192 个可能值。
    - 将 transformer 的上下文大小减少了 192 倍，而视觉质量没有大幅下降
- 阶段2：训练一个自回归transformer 	
    - 将256个BPE编码的文本token，与32*32=1024个图像token连接起来
    - 训练一个自回归transformer，建模文本和图像token的联合分布

**整个过程相当于最大化证据下限 evidence lower bound (ELB) (Kingma & Welling, 2013; Rezende et al., 2014)**

对于模型在图像x、文本描述y、图像token z上的联合似然分布，做因式分解

![因式分解1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729103044.png)

得到下界

![因式分解2](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729103053.png)

### 预备知识

![预备知识1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729103217.png)

    AE – VAE – ELB
    自编码器 Auto-Encoder，AE
    编码器：输入xi，压缩成zi
    解码器：输入zi，还原成yi
    误差 e = xi - yi
    对于图像，计算像素的均方误差MSE
    AE做的是重建，zi与xi是对应的

    Variational AutoEncoder (VAE)，为了生成新图，需要改变zi
    如何改变：在xi编码后的空间里加一个概率分布（比如高斯分布），在其中随机采样zi
    编码器：输入xi，输出x的均值和方差
    在均值和方差对应的高斯分布中采样得到zi
    解码器：输入zi，还原成yi
    如果误差还是xi-yi，那么模型倾向于生成不变的结果，学习到方差为零，VAE退化成AE
    为了保证生成能力，也就是希望xi到zi都符合标准高斯分布，方差不为零
    新的误差 = 重建loss + KL散度
    KL散度用于计算 x均值方差的分布、标准高斯分布 两者的相似度

--- 

    数学建模
    数据样本 x = {xi}
    假设是独立同分布的，目的是生成更多符合该分布的新数据
    需要估计这些样本的分布
    假设概率分布有表达式，需要估计表达式的参数
    极大化似然估计（使表达式对于尽量多的样本适用）

    因为图像数据的分布难以用表达式描述，不能直接估计参数
    间接记为 xi 服从 p(x; θ) 分布，θ是要估计的参数


**似然函数 → 取对数 → 求偏导 → 解方程**

![数学建模1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729103301.png)

![数学建模2](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729103308.png)

没有明确的表达式，无法求解
假设θ服从另一个分布，引入新的变量，近似求解

![近似求解1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729103423.png)

![近似求解3](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729103523.png)

![近似求解2](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729103509.png)


    对应VAE
    p(z)是隐空间的先验分布
    p(x|z;φ) 是解码器
    用q(z|x; θ)表示编码器
    最后生成结果是p(x)

![对应VAE1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729103608.png)


希望q和p的分布尽量接近，用KL散度描述（最小化）

第一项是是重建Loss，第二项就是我们为了引入生成能力而对Latent空间加的约束

![KL散度](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729103622.png)


![公式1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729103703.png)

![公式2](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729103719.png)


![公式3](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729103743.png)

左边的第一项是要最大化似然，第二项也是要最大化（KL散度最小化取负）
最大化左侧，等价于最大化右侧，优化目标就转变成了

![公式4](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729103812.png)


### 阶段1：训练一个离散变分自动编码器 (dVAE)

![dVAE1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729104005.png)

![dVAE2](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729104021.png)


- 只用图像训练
- VAE的编码器、解码器结构是卷积ResNet
- 将 256×256 RGB 图像压缩成 32×32个token

--- 
    ELB难以优化，因为qφ是个离散的分布，
    one-hot编码，argmax 不可导
    我们使用 gumbel-softmax relaxation
    向softmax中引入超参数t 使argmax可导
    实际图像的像素有范围，而VAE概率分布是实数
    pθ用log-laplace分布估计，得到值域 (0,1)
    ELB使用Adam优化

**训练稳定的重点**

    t设置为1/16
    在编码器末尾、解码器开头使用1*1 卷积
    对编码器、解码器resblocks的输出乘以一个小的常数，确保初始化的时候训练稳定
    KL权重β增加到6.6，有利于codebook的使用、重建误差更小


### 阶段2：训练一个自回归transformer
- 学习文本图像token的先验分布，
- 120亿参数的稀疏transformer，只有解码器

- 对于一个文本-图像对
    - 对文本用BPE编码，最多256个token，词典大小16384
    - 对图像编码得到32*32=1024个token，词典大小8192
    - 图像token是阶段1的dVAE编码器提供的，argmax采样，不添加gumbel噪音
    - 两个token拼接到一起，作为一个数据流，自回归的建模

- 每个图像token可以在64个自注意力层与所有文本token关联
    - 文本的注意力mask 是 standard causal mask
    - 图像的注意力mask是 行、列或卷积注意力mask

- 限制文本到256个token，不确定文本和图像token的连接处应该怎样填充
    - 我们对256个位置分别学习了一个填充token，在没有文本时用于填充

- 正则化交叉熵损失，对文本和图像token，用一个batch数据的类别数
    - 因为侧重于图像建模，对文本的交叉熵损失乘以1/8，图像的乘以7/8


### 数据收集
Conceptual Captions数据集：有330万文本图片对，作为MS-COCO的扩展
为了扩大模型参数到120亿，建立了规模类似JFT-300M的数据集，收集网络的2.5亿文本图片对

### 混合精度训练
为了节省现存、提高效率，大部分参数使用16位精度存储，难点在于低精度存储如何训练大参数量而不发散
问题根源在于16位梯度的下溢underflow

这里介绍重点的一项技术：每个残差块的梯度缩放（per-resblock gradient scaling）
下溢：从前一个残差块res-block到后一个，随着模型变深变宽，激活梯度的指数可能低于16位范围的最低值，接近0
消除下溢有助于训练稳定收敛
以往的方法是限制梯度值的范围，这个范围对于文生图模型来说太小了
我们对每个残差块使用单独的梯度缩放比例
 

### 分布式优化

![分布式优化1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729104224.png)

120亿参数的模型需要24GB显存（16-bit精度），超出NVIDIA V100 GPU 16GB

使用参数分片 parameter sharding (Rajbhandari et al., 2019).

- 训练集群上，机器之间的带宽小于同一台机器的GPU带宽
- 训练瓶颈在于all-reduce（平均各个机器的梯度）
- 使用PowerSGD (Vogels et al., 2019)，压缩梯度
- 每个GPU独立计算参数分片梯度的低阶因子
- 误差设置为8个GPU的平均梯度和从低阶因子解压的梯度的残差


### 样本生成
- 用对比模型给文本和图像打分
- 候选图片越多，评分后排序的top k 结果越好

![样本生成](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729174307.png)


## 实验结果

### 量化的结果
零样本评估DALL E，对比了三个以往的工作：AttnGAN，DM-GAN，DF-GAN（图表3）
人工评估，对比DF-GAN（图表7）

![量化的结果1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729174353.png)

![量化的结果2](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729174411.png)

    IS 和 FID 分数（基于Inception Net-V3，输入图片，输出向量，衡量生成图片的质量和多样性）

    9a 在MS-COCO高于以往2个点
    用dVAE编码器的token训练，可以让模型识别低频轮廓，但是这种压缩不利于产生高频细节
    为了定量评估，在9a添加不同半径的高斯滤波器（模糊半径）

    9b 在CUB数据集则是非常糟糕，有40个点的差距
    We speculate that our zero-shot approach is less likely to compare favorably on specialized distributions such as CUB. 
    We believe that fine-tuning is a promising direction for improvement, and leave this investigation to future work.

    我的想法：CUB图片主要是鸟类近距离特写，对细节的要求很高，几乎没有背景
    DALL-E在原理上不擅长生成高频细节
    网络图片如果不经过crop，会带有很多背景

    9c 如果加上对比模型打分排序，FID会降低


![量化的结果](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729174703.png)


### 数据重叠分析
专门训练一个计算图片相似度的模型
对每张图片，找出最相似的图片
人工检查结果，设置一个阈值，决定哪些图片应该被删除

### 定性发现
    模型具有在抽象层次上组合概念的能力
    2a手风琴+貘
    能够实现组合的泛化
    2c圣诞毛衣+小刺猬+遛狗
    具有零样本的图片转换能力，由自然语言控制
    2d画出速写sketch

    另外还有，改变颜色、风格
    某些转换体现出模型具有目标分割的能力

![定性发现](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729174633.png)


## 结论

文生图，自回归的transformer	
规模可以改善内容生成：
零样本表现，相对于以往的特定模型
一个生成模型集成了很多功能


## 附录

- A discrete VAE的细节
- 结构，训练，对数拉普拉斯分布
- B transformer的细节
- 结构，训练
- C 数据收集的细节
- D 混合精度训练的指导
- E 分布式优化的细节
- 带宽分析，实施细节
- F 人工评估实验的细节

- G 零样本图像转换
- 速写，翻转，近距离特写，
变成红色，戴上墨镜，邮票画风

![附录](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729174616.png)


## 我的点评
文生图的比较初始版本的论文。

